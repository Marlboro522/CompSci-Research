@article{azharApproxRMReducingEnergy2023,
  title = {Approx-{{RM}}: {{Reducing Energy}} on {{Heterogeneous Multicore Processors}} under {{Accuracy}} and {{Timing Constraints}}},
  shorttitle = {Approx-{{RM}}},
  author = {Azhar, Muhammad Waqar and Manivannan, Madhavan and Stenstr{\"o}m, Per},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {44:1--44:25},
  issn = {1544-3566},
  doi = {10.1145/3605214},
  urldate = {2024-09-08},
  abstract = {Reducing energy consumption while providing performance and quality guarantees is crucial for computing systems ranging from battery-powered embedded systems to data centers. This article considers approximate iterative applications executing on heterogeneous multi-core platforms under user-specified performance and quality targets. We note that allowing a slight yet bounded relaxation in solution quality can considerably reduce the required iteration count and thereby can save significant amounts of energy. To this end, this article proposes Approx-RM, a resource management scheme that reduces energy expenditure while guaranteeing a specified performance as well as accuracy target. Approx-RM predicts the number of iterations required to meet the relaxed accuracy target at runtime. The time saved generates execution-time slack, which allows Approx-RM to allocate fewer resources on a heterogeneous multi-core platform in terms of DVFS, core type, and core count to save energy while meeting the performance target. Approx-RM contributes with lightweight methods for predicting the iteration count needed to meet the accuracy target and the resources needed to meet the performance target. Approx-RM uses the aforementioned predictions to allocate just enough resources to comply with quality of service constraints to save energy. Our evaluation shows energy savings of 31.6\%, on average, compared to Race-to-idle when the accuracy is only relaxed by 1\%. Approx-RM incurs timing and energy overheads of less than 0.1\%.},
  note = {15 secs, Trashit, Not related.},
  file = {/Users/ilu/Zotero/storage/ZBX7K9TJ/Azhar et al. - 2023 - Approx-RM Reducing Energy on Heterogeneous Multicore Processors under Accuracy and Timing Constrain.pdf}
}

@article{benmezianeMultiobjectiveHardwareawareNeural2023,
  title = {Multi-Objective {{Hardware-aware Neural Architecture Search}} with {{Pareto Rank-preserving Surrogate Models}}},
  author = {Benmeziane, Hadjer and Ouarnoughi, Hamza and El Maghraoui, Kaoutar and Niar, Smail},
  year = {2023},
  month = apr,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {29:1--29:21},
  issn = {1544-3566},
  doi = {10.1145/3579853},
  urldate = {2024-09-08},
  abstract = {Deep learning (DL) models such as convolutional neural networks (ConvNets) are being deployed to solve various computer vision and natural language processing tasks at the edge. It is a challenge to find the right DL architecture that simultaneously meets the accuracy, power, and performance budgets of such resource-constrained devices. Hardware-aware Neural Architecture Search (HW-NAS) has recently gained steam by automating the design of efficient DL models for a variety of target hardware platforms. However, such algorithms require excessive computational resources. Thousands of GPU days are required to evaluate and explore an architecture search space such as FBNet\&nbsp;[45]. State-of-the-art approaches propose using surrogate models to predict architecture accuracy and hardware performance to speed up HW-NAS. Existing approaches use independent surrogate models to estimate each objective, resulting in non-optimal Pareto fronts. In this article, HW-PR-NAS,1 a novel Pareto rank-preserving surrogate model for edge computing platforms, is presented. Our model integrates a new loss function that ranks the architectures according to their Pareto rank, regardless of the actual values of the various objectives. We employ a simple yet effective surrogate model architecture that can be generalized to any standard DL model. We then present an optimized evolutionary algorithm that uses and validates our surrogate model. Our approach has been evaluated on seven edge hardware platforms from various classes, including ASIC, FPGA, GPU, and multi-core CPU. The evaluation results show that HW-PR-NAS achieves up to 2.5{\texttimes} speedup compared to state-of-the-art methods while achieving 98\% near the actual Pareto front.},
  note = {20 secs, Trashit, Not related.},
  file = {/Users/ilu/Zotero/storage/LQSTIW6N/Benmeziane et al. - 2023 - Multi-objective Hardware-aware Neural Architecture Search with Pareto Rank-preserving Surrogate Mode.pdf}
}

@article{chenFlexPointerFastAddress2023,
  title = {{{FlexPointer}}: {{Fast Address Translation Based}} on {{Range TLB}} and {{Tagged Pointers}}},
  shorttitle = {{{FlexPointer}}},
  author = {Chen, Dongwei and Tong, Dong and Yang, Chun and Yi, Jiangfang and Cheng, Xu},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {30:1--30:24},
  issn = {1544-3566},
  doi = {10.1145/3579854},
  urldate = {2024-09-08},
  abstract = {Page-based virtual memory relies on TLBs to accelerate the address translation. Nowadays, the gap between application workloads and the capacity of TLB continues to grow, bringing many costly TLB misses and making the TLB a performance bottleneck. Previous studies seek to narrow the gap by exploiting the contiguity of physical pages. One promising solution is to group pages that are both virtually and physically contiguous into a memory range. Recording range translations can greatly increase the TLB reach, but ranges are also hard to index because they have arbitrary bounds. The processor has to compare against all the boundaries to determine which range an address falls in, which restricts the usage of memory ranges. In this article, we propose a tagged-pointer-based scheme, FlexPointer, to solve the range indexing problem. The core insight of FlexPointer is that large memory objects are rare, so we can create memory ranges based on such objects and assign each of them a unique ID. With the range ID integrated into pointers, we can index the range TLB with IDs and greatly simplify its structure. Moreover, because the ID is stored in the unused bits of a pointer and is not manipulated by the address generation, we can shift the range lookup to an earlier stage, working in parallel with the address generation. According to our trace-based simulation results, FlexPointer can reduce nearly all the L1 TLB misses, and page walks for a variety of memory-intensive workloads. Compared with a 4K-page baseline system, FlexPointer shows a 14\% performance improvement on average and up to 2.8x speedup in the best case. For other workloads, FlexPointer shows no performance degradation.},
  note = {50 secs, Scan it, Address translation methodology in TLB.},
  file = {/Users/ilu/Zotero/storage/YMKEP54T/Chen et al. - 2023 - FlexPointer Fast Address Translation Based on Range TLB and Tagged Pointers.pdf}
}

@article{chenJointlyOptimizingJob2023,
  title = {Jointly {{Optimizing Job Assignment}} and {{Resource Partitioning}} for {{Improving System Throughput}} in {{Cloud Datacenters}}},
  author = {Chen, Ruobing and Shi, Haosen and Wu, Jinping and Li, Yusen and Liu, Xiaoguang and Wang, Gang},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {34:1--34:24},
  issn = {1544-3566},
  doi = {10.1145/3593055},
  urldate = {2024-09-08},
  abstract = {Colocating multiple jobs on the same server has been widely applied for improving resource utilization in cloud datacenters. However, the colocated jobs would contend for the shared resources, which could lead to significant performance degradation. An efficient approach for eliminating performance interference is to partition the shared resources among the colocated jobs. However, this makes the resource management in datacenters very challenging. In this paper, we propose JointOPT, the first resource management framework that optimizes job assignment and resource partitioning jointly for improving the throughput of cloud datacenters. JointOPT uses a local search based algorithm to find the near optimal job assignment configuration, and uses a deep reinforcement learning (DRL) based approach to dynamically partition the shared resources among the colocated jobs. In order to reduce the interaction overhead with real systems, it leverages deep learning to estimate job performance without running them on real servers. We conduct extensive experiments to evaluate JointOPT and the results show that JointOPT significantly outperforms the state-of-the-art baselines, with an advantage from 13.3\% to 47.7\%.},
  note = {40secs, Trashit, Not related.},
  file = {/Users/ilu/Zotero/storage/XEHFMXWW/Chen et al. - 2023 - Jointly Optimizing Job Assignment and Resource Partitioning for Improving System Throughput in Cloud.pdf}
}

@article{chenLockFreeHighperformanceHashing2022,
  title = {Lock-{{Free High-performance Hashing}} for {{Persistent Memory}} via {{PM-aware Holistic Optimization}}},
  author = {Chen, Zhangyu and Hua, Yu and Ding, Luochangqi and Ding, Bo and Zuo, Pengfei and Liu, Xue},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {5:1--5:26},
  issn = {1544-3566},
  doi = {10.1145/3561651},
  urldate = {2024-09-08},
  abstract = {Persistent memory (PM) provides large-scale non-volatile memory (NVM) with DRAM-comparable performance. The non-volatility and other unique characteristics of PM architecture bring new opportunities and challenges for the efficient storage system design. For example, some recent crash-consistent and write-friendly hashing schemes are proposed to provide fast queries for PM systems. However, existing PM hashing indexes suffer from the concurrency bottleneck due to the blocking resizing and expensive lock-based concurrency control for queries. Moreover, the lack of PM awareness and systematical design further increases the query latency. To address the concurrency bottleneck of lock contention in PM hashing, we propose clevel hashing, a lock-free concurrent level hashing scheme that provides non-blocking resizing via background threads and lock-free search/insertion/update/deletion using atomic primitives to enable high concurrency for PM hashing. By exploiting the PM characteristics, we present a holistic approach to building clevel hashing for high throughput and low tail latency via the PM-aware index/allocator co-design. The proposed volatile announcement array with a helping mechanism coordinates lock-free insertions and guarantees a strong consistency model. Our experiments using real-world YCSB workloads on Intel Optane DC PMM show that clevel hashing, respectively, achieves up to 5.7{\texttimes} and 1.6{\texttimes} higher throughput than state-of-the-art P-CLHT and Dash while guaranteeing low tail latency, e.g., 1.9{\texttimes}--7.2{\texttimes} speedup for the p99 latency with the insert-only workload.},
  note = {30 sec, Trash it, Not related.},
  file = {/Users/ilu/Zotero/storage/WTCHBS79/Chen et al. - 2022 - Lock-Free High-performance Hashing for Persistent Memory via PM-aware Holistic Optimization.pdf}
}

@article{duFastOneSidedRDMABased2023,
  title = {Fast {{One-Sided RDMA-Based State Machine Replication}} for {{Disaggregated Memory}}},
  author = {Du, Jingwen and Wang, Fang and Feng, Dan and Gan, Changchen and Cao, Yuchao and Zou, Xiaomin and Li, Fan},
  year = {2023},
  month = apr,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {31:1--31:25},
  issn = {1544-3566},
  doi = {10.1145/3587096},
  urldate = {2024-09-08},
  abstract = {Disaggregated memory architecture has risen in popularity for large datacenters with the advantage of improved resource utilization, failure isolation, and elasticity. Replicated state machines (RSMs) have been extensively used for reliability and consistency. In traditional RSM protocols, each replica stores replicated data and has the computing power to participate in some part of the protocols. However, traditional RSM protocols fail to work in the disaggregated memory architecture due to asymmetric resources on CPU nodes and memory nodes. This article proposes ECHO, a fast one-sided RDMA-based RSM protocol with lightweight log replication and remote applying, efficient linearizability guarantee, and fast coordinator failure recovery. ECHO enables all operations in the protocol to be efficiently executed using only one-sided RDMA, without the participation of any computing resource in the memory pool. To provide lightweight log replication and remote applying, ECHO couples the replicated log and the state machine to avoid dual-copy and performs remote applying by updating pointers. To enable efficient remote log state management, ECHO leverages a hitchhiked log state updating scheme to eliminate extra network round trips. To provide efficient linearizability guarantee, ECHO performs immediate remote applying after log replication and leverages the local locks at the coordinator to ensure linear consistency. Moreover, ECHO adopts a commit-aware log cache to make data visible immediately after being committed. To achieve fast failure recovery, ECHO leverages a commit point identification scheme to reduce the overhead of log consistency recovery. Experimental results demonstrate that ECHO outperforms the state-of-the-art RSM protocol (namely Sift) in multiple scenarios. For example, ECHO achieves 27\%--52\% higher throughput on typical write-intensive workloads. Moreover, ECHO reduces the consistency recovery time by three orders of magnitude for coordinator failure.},
  file = {/Users/ilu/Zotero/storage/R2M9JNHM/Du et al. - 2023 - Fast One-Sided RDMA-Based State Machine Replication for Disaggregated Memory.pdf}
}

@article{erisPuppeteerRandomForest2022,
  title = {Puppeteer: {{A Random Forest Based Manager}} for {{Hardware Prefetchers Across}} the {{Memory Hierarchy}}},
  shorttitle = {Puppeteer},
  author = {Eris, Furkan and Louis, Marcia and Eris, Kubra and Abell{\'a}n, Jos{\'e} and Joshi, Ajay},
  year = {2022},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {19:1--19:25},
  issn = {1544-3566},
  doi = {10.1145/3570304},
  urldate = {2024-09-08},
  abstract = {Over the years, processor throughput has steadily increased. However, the memory throughput has not increased at the same rate, which has led to the memory wall problem in turn increasing the gap between effective and theoretical peak processor performance. To cope with this, there has been an abundance of work in the area of data/instruction prefetcher designs. Broadly, prefetchers predict future data/instruction address accesses and proactively fetch data/instructions in the memory hierarchy with the goal of lowering data/instruction access latency. To this end, one or more prefetchers are deployed at each level of the memory hierarchy, but typically, each prefetcher gets designed in isolation without comprehensively accounting for other prefetchers in the system. As a result, individual prefetchers do not always complement each other, and that leads to lower average performance gains and/or many negative outliers. In this work, we propose Puppeteer, which is a hardware prefetcher manager that uses a suite of random forest regressors to determine at runtime which prefetcher should be ON at each level in the memory hierarchy, such that the prefetchers complement each other and we reduce the data/instruction access latency. Compared to a design with no prefetchers, using Puppeteer \&nbsp;we improve IPC by 46.0\% in 1 one-core, 25.8\% in four-core, and 11.9\% in eight-core processors on average across traces generated from SPEC2017, SPEC2006, and Cloud suites with {\textasciitilde}11-KB overhead. Moreover, we also reduce the number of negative outliers by more than 89\%, and the performance loss of the worst-case negative outlier from 25\% to only 5\% compared to the state of the art.},
  note = {30 secs, Scan it, a new IF block block design for each level of memory, related to your approach of solving your problem of branch prediction},
  file = {/Users/ilu/Zotero/storage/YCYSQ2BW/Eris et al. - 2022 - Puppeteer A Random Forest Based Manager for Hardware Prefetchers Across the Memory Hierarchy.pdf}
}

@article{espindolaSourceMatchingRewriting2023,
  title = {Source {{Matching}} and {{Rewriting}} for {{MLIR Using String-Based Automata}}},
  author = {Espindola, Vinicius and Zago, Luciano and Yviquel, Herv{\'e} and Araujo, Guido},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {22:1--22:26},
  issn = {1544-3566},
  doi = {10.1145/3571283},
  urldate = {2024-09-08},
  abstract = {A typical compiler flow relies on a uni-directional sequence of translation/optimization steps that lower the program abstract representation, making it hard to preserve higher-level program information across each transformation step. On the other hand, modern ISA extensions and hardware accelerators can benefit from the compiler's ability to detect and raise program idioms to acceleration instructions or optimized library calls. Although recent works based on Multi-Level IR (MLIR) have been proposed for code raising, they rely on specialized languages, compiler recompilation, or in-depth dialect knowledge. This article presents Source Matching and Rewriting (SMR), a user-oriented source-code-based approach for MLIR idiom matching and rewriting that does not require a compiler expert's intervention. SMR uses a two-phase automaton-based DAG-matching algorithm inspired by early work on tree-pattern matching. First, the idiom Control-Dependency Graph (CDG) is matched against the program's CDG to rule out code fragments that do not have a control-flow structure similar to the desired idiom. Second, candidate code fragments from the previous phase have their Data-Dependency Graphs (DDGs) constructed and matched against the idiom DDG. Experimental results show that SMR can effectively match idioms from Fortran (FIR) and C (CIL) programs while raising them as BLAS calls to improve performance. Additional experiments also show performance improvements when using SMR to enable code replacement in areas like approximate computing and hardware acceleration.},
  note = {2:00, Trash it, talks about MLIR in the perspective of compilers, and translators.},
  file = {/Users/ilu/Zotero/storage/LAN44DCC/Espindola et al. - 2023 - Source Matching and Rewriting for MLIR Using String-Based Automata.pdf}
}

@article{gondimallaOccamOptimalData2022,
  title = {Occam: {{Optimal Data Reuse}} for {{Convolutional Neural Networks}}},
  shorttitle = {Occam},
  author = {Gondimalla, Ashish and Liu, Jianqiao and Thottethodi, Mithuna and Vijaykumar, T. N.},
  year = {2022},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {12:1--12:25},
  issn = {1544-3566},
  doi = {10.1145/3566052},
  urldate = {2024-09-08},
  abstract = {Convolutional neural networks (CNNs) are emerging as powerful tools for image processing in important commercial applications. We focus on the important problem of improving the latency of image recognition. While CNNs are highly amenable to prefetching and multithreading to avoid memory latency issues, CNNs' large data -- each layer's input, filters, and output -- poses a memory bandwidth problem. While previous work captures only some of the enormous data reuse, full reuse implies that the initial input image and filters are read once from off-chip and the final output is written once off-chip without spilling the intermediate layers' data to off-chip. We propose Occam to capture full reuse via four contributions. First, we identify the necessary conditions for full reuse. Second, we identify the dependence closure as the sufficient condition to capture full reuse using the least on-chip memory. Third, because the dependence closure is often too large to fit in on-chip memory, we propose a dynamic programming algorithm that optimally partitions a given CNN to guarantee the least off-chip traffic at the partition boundaries for a given on-chip capacity. While tiling is well-known, our contribution determines the optimal cross-layer tiles. Occam's partitions reside on different chips, forming a pipeline so that a partition's filters and dependence closure remain on-chip as different images pass through (i.e., each partition incurs off-chip traffic only for its inputs and outputs). Finally, because the optimal partitions may result in an unbalanced pipeline, we propose staggered asynchronous pipelines (STAPs) that replicate bottleneck stages to improve throughput by staggering mini-batches across replicas. Importantly, STAPs achieve balanced pipelines without changing Occam's optimal partitioning. Our simulations show that, on average, Occam cuts off-chip transfers by 21{\texttimes} and achieves 2.04{\texttimes} and 1.21{\texttimes} better performance, and 33\% better energy than the base case, respectively. Using a field-programmable gate array (FPGA) implementation, Occam performs 6.1{\texttimes} and 1.5{\texttimes} better, on average, than the base case and Layer Fusion, respectively.},
  note = {12 secs, Trash it, Not related.},
  file = {/Users/ilu/Zotero/storage/XV2W8EEY/Gondimalla et al. - 2022 - Occam Optimal Data Reuse for Convolutional Neural Networks.pdf}
}

@article{huangSplitZNSEfficientLSMTree2023,
  title = {{{SplitZNS}}: {{Towards}} an {{Efficient LSM-Tree}} on {{Zoned Namespace SSDs}}},
  shorttitle = {{{SplitZNS}}},
  author = {Huang, Dong and Feng, Dan and Liu, Qiankun and Ding, Bo and Zhao, Wei and Wei, Xueliang and Tong, Wei},
  year = {2023},
  month = aug,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {45:1--45:26},
  issn = {1544-3566},
  doi = {10.1145/3608476},
  urldate = {2024-09-08},
  abstract = {The Zoned Namespace (ZNS) Solid State Drive (SSD) is a nascent form of storage device that offers novel prospects for the Log Structured Merge Tree (LSM-tree). ZNS exposes erase blocks in SSD as append-only zones, enabling the LSM-tree to gain awareness of the physical layout of data. Nevertheless, LSM-tree on ZNS SSDs necessitates Garbage Collection (GC) owing to the mismatch between the gigantic zones and relatively small Sorted String Tables\&nbsp;(SSTables). Through extensive experiments, we observe that a smaller zone size can reduce data migration in GC at the cost of a significant performance decline owing to inadequate parallelism exploitation. In this article, we present SplitZNS, which introduces small zones by tweaking the zone-to-chip mapping to maximize GC efficiency for LSM-tree on ZNS SSDs. Following the multi-level peculiarity of LSM-tree and the inherent parallel architecture of ZNS SSDs, we propose a number of techniques to leverage and accelerate small zones to alleviate the performance impact due to underutilized parallelism. (1) First, we use small zones selectively to prevent exacerbating write slowdowns and stalls due to their suboptimal performance. (2) Second, to enhance parallelism utilization, we propose SubZone Ring, which employs a per-chip FIFO buffer to imitate a large zone writing style; (3) Read Prefetcher, which prefetches data concurrently through multiple chips during compactions; (4) and Read Scheduler, which assigns query requests the highest priority. We build a prototype integrated with SplitZNS to validate its efficiency and efficacy. Experimental results demonstrate that SplitZNS achieves up to 2.77{\texttimes} performance and reduces data migration considerably compared to the lifetime-based data placement.1},
  note = {1:15. Trashit, Related to SSDs},
  file = {/Users/ilu/Zotero/storage/B8SLJRD3/Huang et al. - 2023 - SplitZNS Towards an Efficient LSM-Tree on Zoned Namespace SSDs.pdf}
}

@article{hurFastFlexibleFPGAbased2023,
  title = {A {{Fast}} and {{Flexible FPGA-based Accelerator}} for {{Natural Language Processing Neural Networks}}},
  author = {Hur, Suyeon and Na, Seongmin and Kwon, Dongup and Kim, Joonsung and Boutros, Andrew and Nurvitadhi, Eriko and Kim, Jangwoo},
  year = {2023},
  month = feb,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {11:1--11:24},
  issn = {1544-3566},
  doi = {10.1145/3564606},
  urldate = {2024-09-08},
  abstract = {Deep neural networks (DNNs) have become key solutions in the natural language processing (NLP) domain. However, the existing accelerators customized for their narrow target models cannot support diverse NLP models. Therefore, naively running complex NLP models on the existing accelerators often leads to very marginal performance improvements. For these reasons, architects are now in dire need of a new accelerator that can run various NLP models while taking its full performance potential. In this article, we propose FlexRun, an FPGA-based modular accelerator to efficiently support diverse and complex NLP models. First, we identify key components commonly used by NLP models and implement them on top of a current state-of-the-art FPGA-based accelerator. Next, FlexRun conducts an in-depth design space exploration to find the best accelerator architecture for a target NLP model. Last, FlexRun automatically reconfigures the accelerator based on the exploration results. Our FlexRun design outperforms the current state-of-the-art FPGA-based accelerator by 1.21{\texttimes}--2.73{\texttimes} and 1.15{\texttimes}--1.50{\texttimes} for BERT and GPT2, respectively. Compared to Nvidia's V100 GPU, FlexRun achieves 2.69{\texttimes} higher performance on average for various BERT and GPT2 models.},
  note = {10 secs, Trash it, Not related.},
  file = {/Users/ilu/Zotero/storage/AVJEPIM6/Hur et al. - 2023 - A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks.pdf}
}

@article{jiangHierarchicalModelParallelism2023,
  title = {Hierarchical {{Model Parallelism}} for {{Optimizing Inference}} on {{Many-core Processor}} via {{Decoupled 3D-CNN Structure}}},
  author = {Jiang, Jiazhi and Huang, Zijian and Huang, Dan and Du, Jiangsu and Chen, Lin and Chen, Ziguan and Lu, Yutong},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {42:1--42:21},
  issn = {1544-3566},
  doi = {10.1145/3605149},
  urldate = {2024-09-08},
  abstract = {The tremendous success of convolutional neural network (CNN) has made it ubiquitous in many fields of human endeavor. Many applications such as biomedical analysis and scientific data analysis involve analyzing volumetric data. This spawns huge demand for 3D-CNN. Although accelerators such as GPU may provide higher throughput on deep learning applications, they may not be available in all scenarios. CPU, especially many-core CPU with non-uniform memory access (NUMA) architecture, remains an attractive choice for deep learning inference in many scenarios.In this article, we propose a distributed inference solution for 3D-CNN that targets on the emerging ARM many-core CPU platform. A hierarchical partition approach is claimed to accelerate 3D-CNN inference by exploiting characteristics of memory and cache on ARM many-core CPU. Based on the hierarchical model partition approach, other optimization techniques such as NUMA-aware thread scheduling and optimization of 3D-img2row convolution are designed to exploit the potential of ARM many-core CPU for 3D-CNN. We evaluate our proposed inference solution with several classic 3D-CNNs: C3D, 3D-resnet34, 3D-resnet50, 3D-vgg11, and P3D. Our experimental results show that our solution can boost the performance of the 3D-CNN inference, and achieve much better scalability, with a negligible fluctuation in accuracy. When employing our 3D-CNN inference solution on ACL libraries, it can outperform naive ACL implementations by 11{\texttimes} to 50{\texttimes} on ARM many-core processor. When employing our 3D-CNN inference solution on NCNN libraries, it can outperform the naive NCNN implementations by 5.2{\texttimes} to 14.2{\texttimes} on ARM many-core processor.},
  note = {2:00, trashit, not related and you couldn't make sense of it.},
  file = {/Users/ilu/Zotero/storage/UBVX7WUN/Jiang et al. - 2023 - Hierarchical Model Parallelism for Optimizing Inference on Many-core Processor via Decoupled 3D-CNN.pdf}
}

@article{jinSpecTerminatorBlockingSpeculative2023a,
  title = {{{SpecTerminator}}: {{Blocking Speculative Side Channels Based}} on {{Instruction Classes}} on {{RISC-V}}},
  shorttitle = {{{SpecTerminator}}},
  author = {Jin, Hai and He, Zhuo and Qiang, Weizhong},
  year = {2023},
  month = feb,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {15:1--15:26},
  issn = {1544-3566},
  doi = {10.1145/3566053},
  urldate = {2024-09-08},
  abstract = {In modern processors, speculative execution has significantly improved the performance of processors, but it has also introduced speculative execution vulnerabilities. Recent defenses are based on the delayed execution to block various speculative side channels, but we show that several of the current state-of-the-art defenses fail to block some of the available speculative side channels, and the current most secure defense introduces a performance overhead of up to 24.5\%.We propose SpecTerminator, the first defense framework based on instruction classes that can comprehensively and precisely block all existing speculative side channels. In SpecTerminator, a novel speculative side channel classification scheme based on the features of secret transmission is proposed, and the sensitive instructions in the speculative window are classified and identified using optimized hardware taint tracking and instruction masking techniques to accurately determine the scope of leakage. Then, according to the execution characteristics of these instructions, dedicated delayed execution strategies, such as TLB request ignoring, selective issue, and extended delay-on-miss, are designed for each type of sensitive instruction to precisely control that these instructions are delayed only in pipeline stages that are at risk of leakage. In contrast to previous defenses based on the Gem5 simulator, we have innovatively implemented defenses against Spectre attacks based on the open-source instruction set RISC-V on an FPGA-accelerated simulation platform that is more similar to real hardware. To evaluate the security of SpecTerminator, we have replicated various existing x86-based Spectre variants on RISC-V. On SPEC 2006, SpecTerminator defends against Spectre attacks based on memory hierarchy side channels with a performance overhead of 2.6\% and against all existing Spectre attacks with a performance overhead of 6.0\%.},
  note = {Critical READ,45 seec, scan it, talks about bblocking SPECTRE attacks in RISCV. Need to get the grip on different types of SPECTRE attacks. You may actually be mitigating only one based on the text. Delayed execution strategies, TLB request ignoring, Classifiation of LOAD instructions in ISA whic are speculative depended baased on the architectural changes., TAble 1 has existing hardware defecnecs against spectre attacks, need to critically read those in hte coming days. Experimental setup is unique instead of using genric simiulation like gem5 using FPGA simulation platform will give more access towards the architectural changes ? this doesn't mean it is biased, it is required for the analysis of the experiment results. I donno BOOM might not have been the best processor to choose, a multiple issue inorder may be would've yielded different results ? Finally, SPECTerminator has a lowe performance ocverheard than other hardwaare or OS based techniques.},
  file = {/Users/ilu/Zotero/storage/YCJYBBBF/Jin et al. - 2023 - SpecTerminator Blocking Speculative Side Channels Based on Instruction Classes on RISC-V.pdf}
}

@article{korostelevYaConvConvolutionLow2023,
  title = {{{YaConv}}: {{Convolution}} with {{Low Cache Footprint}}},
  shorttitle = {{{YaConv}}},
  author = {Korostelev, Ivan and L. De Carvalho, Jo{\~a}o P. and Moreira, Jos{\'e} and Amaral, Jos{\'e} Nelson},
  year = {2023},
  month = feb,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {18:1--18:18},
  issn = {1544-3566},
  doi = {10.1145/3570305},
  urldate = {2024-09-08},
  abstract = {This article introduces YaConv, a new algorithm to compute convolution using GEMM microkernels from a Basic Linear Algebra Subprograms library that is efficient for multiple CPU architectures. Previous approaches either create a copy of each image element for each filter element or reload these elements into cache for each GEMM call, leading to redundant instances of the image elements in cache. Instead, YaConv loads each image element once into the cache and maximizes the reuse of these elements. The output image is computed by scattering results of the GEMM microkernel calls to the correct locations in the output image. The main advantage of this new algorithm---which leads to better performance in comparison to the existing im2col approach on several architectures---is a more efficient use of the memory hierarchy. The experimental evaluation on convolutional layers from PyTorch, along with a parameterized study, indicates an average 24\% speedup over im2col convolution. Increased performance comes as a result of 3{\texttimes} reduction in L3 cache accesses and 2{\texttimes} fewer branch instructions.},
  note = {2:00, Trash it, Kinda related but talks about a new algorithm to compute convolution in cnns.},
  file = {/Users/ilu/Zotero/storage/PCL2EQ22/Korostelev et al. - 2023 - YaConv Convolution with Low Cache Footprint.pdf}
}

@article{krolikRNdNFastQuery2023,
  title = {{{rNdN}}: {{Fast Query Compilation}} for {{NVIDIA GPUs}}},
  shorttitle = {{{rNdN}}},
  author = {Krolik, Alexander and Verbrugge, Clark and Hendren, Laurie},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {41:1--41:25},
  issn = {1544-3566},
  doi = {10.1145/3603503},
  urldate = {2024-09-08},
  abstract = {GPU database systems are an effective solution to query optimization, particularly with compilation and data caching. They fall short, however, in end-to-end workloads, as existing compiler toolchains are too expensive for use with short-running queries. In this work, we define and evaluate a runtime-suitable query compilation pipeline for NVIDIA GPUs that extracts high performance with only minimal optimization. In particular, our balanced approach successfully trades minor slowdowns in execution for major speedups in compilation, even as data sizes increase. We demonstrate performance benefits compared to both CPU and GPU database systems using interpreters and compilers, extending query compilation for GPUs beyond cached use cases.},
  note = {30 secs, Trash it, Not related.},
  file = {/Users/ilu/Zotero/storage/VXVIUIUW/Krolik et al. - 2023 - rNdN Fast Query Compilation for NVIDIA GPUs.pdf}
}

@article{liangQuantifyingResourceContention2023,
  title = {Quantifying {{Resource Contention}} of {{Co-located Workloads}} with the {{System-level Entropy}}},
  author = {Liang, Yi and Zeng, Shaokang and Wang, Lei},
  year = {2023},
  month = feb,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {10:1--10:25},
  issn = {1544-3566},
  doi = {10.1145/3563696},
  urldate = {2024-09-08},
  abstract = {The workload co-location, such as deploying offline analysis workloads with online service workloads on the same node, has become common for modern data centers. Workload co-location deployment improves data center resource utilization significantly. Still, it also introduces resource contention, resulting in the online service's quality of service fluctuation, which we call performance interference. As the online service is a tail-latency-sensitive workload, the tail-latency metric can reflect the performance interference degree of the co-location workloads at the application level. However, to guide system design and evaluation, quantitatively evaluating the resource contention of the co-located workloads at the system level is also essential. This article proposes a novel metric called System-Level Entropy (SLE). As a system-level metric, SLE can measure quantitatively resource contention of the co-location systems and perform the apples-to-apples comparison between systems. The experimental results show that SLE can accurately reflect the performance interference of workloads and then evaluate the system resource contention. We also demonstrate two case studies of the SLE. We quantify the affinity of different co-location combinations, including three online services and five offline workloads. Furthermore, we evaluate the effects of state-of-the-art isolated mechanisms (the container and the CPU--affinity binding) with these co-location combinations.},
  note = {30 secs, Trash it, Resource utilization improvement in key-value like data wrt Datacenters.},
  file = {/Users/ilu/Zotero/storage/D48IS9FN/Liang et al. - 2023 - Quantifying Resource Contention of Co-located Workloads with the System-level Entropy.pdf}
}

@article{liuUnifiedBufferCompiling2023,
  title = {Unified {{Buffer}}: {{Compiling Image Processing}} and {{Machine Learning Applications}} to {{Push-Memory Accelerators}}},
  shorttitle = {Unified {{Buffer}}},
  author = {Liu, Qiaoyi and Setter, Jeff and Huff, Dillon and Strange, Maxwell and Feng, Kathleen and Horowitz, Mark and Raina, Priyanka and Kjolstad, Fredrik},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {26:1--26:26},
  issn = {1544-3566},
  doi = {10.1145/3572908},
  urldate = {2024-09-08},
  abstract = {Image processing and machine learning applications benefit tremendously from hardware acceleration. Existing compilers target either FPGAs, which sacrifice power and performance for programmability, or ASICs, which become obsolete as applications change. Programmable domain-specific accelerators, such as coarse-grained reconfigurable arrays (CGRAs), have emerged as a promising middle-ground, but they have traditionally been difficult compiler targets since they use a different memory abstraction. In contrast to CPUs and GPUs, the memory hierarchies of domain-specific accelerators use push memories: memories that send input data streams to computation kernels or to higher or lower levels in the memory hierarchy and store the resulting output data streams. To address the compilation challenge caused by push memories, we propose that the representation of these memories in the compiler be altered to directly represent them by combining storage with address generation and control logic in a single structure---a unified buffer. The unified buffer abstraction enables the compiler to separate generic push memory optimizations from the mapping to specific memory implementations in the backend. This separation allows our compiler to map high-level Halide applications to different CGRA memory designs, including some with a ready-valid interface. The separation also opens the opportunity for optimizing push memory elements on reconfigurable arrays. Our optimized memory implementation, the Physical Unified Buffer, uses a wide-fetch, single-port SRAM macro with built-in address generation logic to implement a buffer with two read and two write ports. It is 18\% smaller and consumes 31\% less energy than a physical buffer implementation using a dual-port memory that only supports two ports. Finally, our system evaluation shows that enabling a compiler to support CGRAs leads to performance and energy benefits. Over a wide range of image processing and machine learning applications, our CGRA achieves 4.7{\texttimes} better runtime and 3.5{\texttimes} better energy-efficiency compared to an FPGA.},
  note = {2:00, Treshit, Not related.},
  file = {/Users/ilu/Zotero/storage/QDIFRNNI/Liu et al. - 2023 - Unified Buffer Compiling Image Processing and Machine Learning Applications to Push-Memory Accelera.pdf}
}

@article{luinaudSymbolicAnalysisData2022,
  title = {Symbolic {{Analysis}} for {{Data Plane Programs Specialization}}},
  author = {Luinaud, Thomas and Langlois, J. M. Pierre and Savaria, Yvon},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {1:1--1:21},
  issn = {1544-3566},
  doi = {10.1145/3557727},
  urldate = {2024-09-08},
  abstract = {Programmable network data planes have extended the capabilities of packet processing in network devices by allowing custom processing pipelines and agnostic packet processing. While a variety of applications can be implemented on current programmable data planes, there are significant constraints due to hardware limitations. One way to meet these constraints is by optimizing data plane programs. Program optimization can be achieved by specializing code that leverages architectural specificity or by compilation passes. In the case of programmable data planes, to respond to the varying requirements of a large set of applications, data plane programs can target different architectures. This leads to difficulties when developers want to reuse the code. One solution to that is to use compiler optimization techniques. We propose performing data plane program specialization to reduce the generated program size. To this end, we propose to specialize in programs written in P4, a Domain Specific Language (DSL) designed for specifying network data planes. The proposed method takes advantage of key aspects of the P4 language to perform a symbolic analysis on a P4 program and then partially evaluate the program to specialize it. The approach we propose is independent of the target architecture. We evaluate the specialization technique by implementing a packet deparser on an FPGA. The results demonstrate that program specialization can reduce the resource usage by a factor of 2 for various packet deparsers.},
  note = {1:00, traash it, Processing datapackets using FPGA},
  file = {/Users/ilu/Zotero/storage/AQQBHSAV/Luinaud et al. - 2022 - Symbolic Analysis for Data Plane Programs Specialization.pdf}
}

@article{maOptimizedFrameworkMatrix2023,
  title = {An {{Optimized Framework}} for {{Matrix Factorization}} on the {{New Sunway Many-core Platform}}},
  author = {Ma, Wenjing and Liu, Fangfang and Chen, Daokun and Lu, Qinglin and Hu, Yi and Wang, Hongsen and Yuan, Xinhui},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {23:1--23:24},
  issn = {1544-3566},
  doi = {10.1145/3571856},
  urldate = {2024-09-08},
  abstract = {Matrix factorization functions are used in many areas and often play an important role in the overall performance of the applications. In the LAPACK library, matrix factorization functions are implemented with blocked factorization algorithm, shifting most of the workload to the high-performance Level-3 BLAS functions. But the non-blocked part, the panel factorization, becomes the performance bottleneck, especially for small- and medium-size matrices that are the common cases in many real applications. On the new Sunway many-core platform, the performance bottleneck of panel factorization can be alleviated by keeping the panel in the LDM for the panel factorization. Therefore, we propose a new framework for implementing matrix factorization functions on the new Sunway many-core platform, facilitating the in-LDM panel factorization. The framework provides a template class with wrapper functions, which integrates inter-CPE communication for the Level-1 and Level-2 BLAS functions with flexible interfaces and can accommodate different partitioning schemes. With the framework, writing panel factorization code with data residing in the LDM space can be done with much higher productivity. We implemented three functions (dgetrf, dgeqrf, and dpotrf) based on the framework and compared our work with a CPE\_BLAS version, which uses the original LAPACK implementation linked with optimized BLAS library that runs on the CPE mesh. Using the most favorable partitioning, the panel factorization part achieves speedup of up to 26.3, 19.1, and 18.2 for the three matrix factorization functions. For the whole function, our implementation is based on a carefully tuned recursion framework, and we added specific optimization to some subroutines used in the factorization functions. Overall, we obtained average speedup of 9.76 on dgetrf, 10.12 on dgeqrf, and 4.16 on dpotrf, compared to the CPE\_BLAS version. Based on the current template class, our work can be extended to support more categories of linear algebra functions.},
  note = {2:00, Trashit, Talsk about sunway processor as a whole.},
  file = {/Users/ilu/Zotero/storage/4YHD8JNV/Ma et al. - 2023 - An Optimized Framework for Matrix Factorization on the New Sunway Many-core Platform.pdf}
}

@article{mastorasDesignImplementationNonblocking2022,
  title = {Design and {{Implementation}} for {{Nonblocking Execution}} in {{GraphBLAS}}: {{Tradeoffs}} and {{Performance}}},
  shorttitle = {Design and {{Implementation}} for {{Nonblocking Execution}} in {{GraphBLAS}}},
  author = {Mastoras, Aristeidis and Anagnostidis, Sotiris and Yzelman, Albert-Jan N.},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {6:1--6:23},
  issn = {1544-3566},
  doi = {10.1145/3561652},
  urldate = {2024-09-08},
  abstract = {GraphBLASis a recent standard that allows the expression of graph algorithms in the language of linear algebra and enables automatic code parallelization and optimization. GraphBLAS operations are memory bound and may benefit from data locality optimizations enabled by nonblocking execution. However, nonblocking execution remains under-evaluated. In this article, we present a novel design and implementation that investigates nonblocking execution in GraphBLAS. Lazy evaluation enables runtime optimizations that improve data locality, and dynamic data dependence analysis identifies operations that may reuse data in cache. The nonblocking execution of an arbitrary number of operations results in dynamic parallelism, and the performance of the nonblocking execution depends on two parameters, which are automatically determined, at run-time, based on a proposed analytic model. The evaluation confirms the importance of nonblocking execution for various matrices of three algorithms, by showing up to 4.11{\texttimes} speedup over blocking execution as a result of better cache utilization. The proposed analytic model makes the nonblocking execution reach up to 5.13{\texttimes} speedup over the blocking execution. The fully automatic performance is very close to that obtained by using the best manual configuration for both small and large matrices. Finally, the evaluation includes a comparison with other state-of-the-art frameworks for numerical linear algebra programming that employ parallel execution and similar optimizations to those discussed in this work, and the presented nonblocking execution reaches up to 16.1{\texttimes} speedup over the state of the art.},
  note = {35 secs, scan it, could be useful as you can see how the pipeline reacts to multi threading and it's execution flow.},
  file = {/Users/ilu/Zotero/storage/4XKPT5QN/Mastoras et al. - 2022 - Design and Implementation for Nonblocking Execution in GraphBLAS Tradeoffs and Performance.pdf}
}

@article{minerviniVitruviusAreaEfficientRISCV2023,
  title = {Vitruvius+: {{An Area-Efficient RISC-V Decoupled Vector Coprocessor}} for {{High Performance Computing Applications}}},
  shorttitle = {Vitruvius+},
  author = {Minervini, Francesco and Palomar, Oscar and Unsal, Osman and Reggiani, Enrico and Quiroga, Josue and Marimon, Joan and Rojas, Carlos and Figueras, Roger and Ruiz, Abraham and Gonzalez, Alberto and Mendoza, Jonnatan and Vargas, Ivan and Hernandez, C{\'e}sar and Cabre, Joan and Khoirunisya, Lina and Bouhali, Mustapha and Pavon, Julian and Moll, Francesc and Olivieri, Mauro and Kovac, Mario and Kovac, Mate and Dragic, Leon and Valero, Mateo and Cristal, Adrian},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {28:1--28:25},
  issn = {1544-3566},
  doi = {10.1145/3575861},
  urldate = {2024-09-08},
  abstract = {The maturity level of RISC-V and the availability of domain-specific instruction set extensions, like vector processing, make RISC-V a good candidate for supporting the integration of specialized hardware in processor cores for the High Performance Computing (HPC) application domain. In this article,1 we present Vitruvius+, the vector processing acceleration engine that represents the core of vector instruction execution in the HPC challenge that comes within the EuroHPC initiative. It implements the RISC-V vector extension (RVV) 0.7.1 and can be easily connected to a scalar core using the Open Vector Interface standard. Vitruvius+ natively supports long vectors: 256 double precision floating-point elements in a single vector register. It is composed of a set of identical vector pipelines (lanes), each containing a slice of the Vector Register File and functional units (one integer, one floating point). The vector instruction execution scheme is hybrid in-order/out-of-order and is supported by register renaming and arithmetic/memory instruction decoupling. On a stand-alone synthesis, Vitruvius+ reaches a maximum frequency of 1.4 GHz in typical conditions (TT/0.80V/25{$^\circ$}C) using GlobalFoundries 22FDX FD-SOI. The silicon implementation has a total area of 1.3 mm2 and maximum estimated power of {$\sim$}920 mW for one instance of Vitruvius+ equipped with eight vector lanes.},
  note = {2:00, Definite Scan, HPC through modification of ISA RISC-Vfor Exascale systems.},
  file = {/Users/ilu/Zotero/storage/E259TGF3/Minervini et al. - 2023 - Vitruvius+ An Area-Efficient RISC-V Decoupled Vector Coprocessor for High Performance Computing App.pdf}
}

@article{mummidiACTIONAdaptiveCache2023,
  title = {{{ACTION}}: {{Adaptive Cache Block Migration}} in {{Distributed Cache Architectures}}},
  shorttitle = {{{ACTION}}},
  author = {Mummidi, Chandra Sekhar and Kundu, Sandip},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {25:1--25:19},
  issn = {1544-3566},
  doi = {10.1145/3572911},
  urldate = {2024-09-08},
  abstract = {Chip multiprocessors (CMP) with more cores have more traffic to the last-level cache (LLC). Without a corresponding increase in LLC bandwidth, such traffic cannot be sustained, resulting in performance degradation. Previous research focused on data placement techniques to improve access latency in Non-Uniform Cache Architectures (NUCA). Placing data closer to the referring core reduces traffic in cache interconnect. However, earlier data placement work did not account for the frequency with which specific memory references are accessed. The difficulty of tracking access frequency for all memory references is one of the main reasons why it was not considered in NUCA data placement. In this research, we present a hardware-assisted solution called ACTION (Adaptive Cache Block Migration) to track the access frequency of individual memory references and prioritize placement of frequently referred data closer to the affine core. ACTION mechanism implements cache block migration when there is a detectable change in access frequencies due to a shift in the program phase. ACTION counts access references in the LLC stream using a simple and approximate method and uses a straightforward placement and migration solution to keep the hardware overhead low. We evaluate ACTION on a 4-core CMP with a 5x5 mesh LLC network implementing a partitioned D-NUCA against workloads exhibiting distinct asymmetry in cache block access frequency. Our simulation results indicate that ACTION can improve CMP performance by up to 7.5\% over state-of-the-art (SOTA) D-NUCA solutions.},
  note = {45 sec, SCan it, Could be potentially used to mitigate Spec. Execution.},
  file = {/Users/ilu/Zotero/storage/DTPQMELD/Mummidi and Kundu - 2023 - ACTION Adaptive Cache Block Migration in Distributed Cache Architectures.pdf}
}

@article{olgunPiDRAMHolisticEndend2022,
  title = {{{PiDRAM}}: {{A Holistic End-to-end FPGA-based Framework}} for {{Processing-in-DRAM}}},
  shorttitle = {{{PiDRAM}}},
  author = {Olgun, Ataberk and Luna, Juan G{\'o}mez and Kanellopoulos, Konstantinos and Salami, Behzad and Hassan, Hasan and Ergin, Oguz and Mutlu, Onur},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {8:1--8:31},
  issn = {1544-3566},
  doi = {10.1145/3563697},
  urldate = {2024-09-08},
  abstract = {Commodity DRAM-based processing-using-memory (PuM) techniques that are supported by off-the-shelf DRAM chips present an opportunity for alleviating the data movement bottleneck at low cost. However, system integration of these techniques imposes non-trivial challenges that are yet to be solved. Potential solutions to the integration challenges require appropriate tools to develop any necessary hardware and software components. Unfortunately, current proprietary computing systems, specialized DRAM-testing platforms, or system simulators do not provide the flexibility and/or the holistic system view that is necessary to properly evaluate and deal with the integration challenges of commodity DRAM-based PuM techniques.We design and develop Processing-in-DRAM (PiDRAM), the first flexible end-to-end framework that enables system integration studies and evaluation of real, commodity DRAM-based PuM techniques. PiDRAM provides software and hardware components to rapidly integrate PuM techniques across the whole system software and hardware stack. We implement PiDRAM on an FPGA-based RISC-V system. To demonstrate the flexibility and ease of use of PiDRAM, we implement and evaluate two state-of-the-art commodity DRAM-based PuM techniques: (i) in-DRAM copy and initialization (RowClone) and (ii) in-DRAM true random number generation (D-RaNGe). We describe how we solve key integration challenges to make such techniques work and be effective on a real-system prototype, including memory allocation, alignment, and coherence. We observe that end-to-end RowClone speeds up bulk copy and initialization operations by 14.6{\texttimes} and 12.6{\texttimes}, respectively, over conventional CPU copy, even when coherence is supported with inefficient cache flush operations. Over PiDRAM's extensible codebase, integrating both RowClone and D-RaNGe end-to-end on a real RISC-V system prototype takes only 388 lines of Verilog code and 643 lines of C++ code.},
  note = {58sec, Scan it, Near memory computing paradigm using simulators, could be an interesting study.},
  file = {/Users/ilu/Zotero/storage/T8MDLUGN/Olgun et al. - 2022 - PiDRAM A Holistic End-to-end FPGA-based Framework for Processing-in-DRAM.pdf}
}

@article{pengFlexHMPracticalSystem2022,
  title = {{{FlexHM}}: {{A Practical System}} for {{Heterogeneous Memory}} with {{Flexible}} and {{Efficient Performance Optimizations}}},
  shorttitle = {{{FlexHM}}},
  author = {Peng, Bo and Dong, Yaozu and Yao, Jianguo and Wu, Fengguang and Guan, Haibing},
  year = {2022},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {13:1--13:26},
  issn = {1544-3566},
  doi = {10.1145/3565885},
  urldate = {2024-09-08},
  abstract = {With the rapid development of cloud computing, numerous cloud services, containers, and virtual machines have been bringing tremendous demands on high-performance memory resources to modern data centers. Heterogeneous memory, especially the newly released Optane memory, offer appropriate alternatives against DRAM in clouds with the advantages of larger capacity, lower purchase cost, and promising performance. However, cloud services suffer serious implementation inconvenience and performance degradation when using hybrid DRAM and Optane memory. This article proposes FlexHM, a practical system to manage transparent heterogeneous memory resources and flexibly optimize memory access performance for all VMs, containers, and native applications. We present an open-source prototype of FlexHM in Linux with several main contributions. First, FlexHM raises a novel two-level NUMA design to manage DRAM and Optane memory as transparent main memory resources. Second, FlexHM provides flexible and efficient memory management, helping optimize memory access performance or save purchase costs of memory resources for differential cloud services with customized management strategies. Finally, the evaluations show that cloud workloads using 50\% Optane slow memory on FlexHM can achieve up to 93\% of the performance when using all-DRAM, and FlexHM provides up to 5.8{\texttimes} improvement over the previous heterogeneous memory system solution when workloads use the same ratio of DRAM and Optane memory.},
  note = {1:30, scan it, Talks about memory optimization for performance gains using Flexhm in optane memory system and NVM. Diversity in testing with benchmarks. Experimental set up not biased.},
  file = {/Users/ilu/Zotero/storage/V2GEGQVH/Peng et al. - 2022 - FlexHM A Practical System for Heterogeneous Memory with Flexible and Efficient Performance Optimiza.pdf}
}

@article{perezUserdrivenOnlineKernel2023,
  title = {User-Driven {{Online Kernel Fusion}} for {{SYCL}}},
  author = {P{\'e}rez, V{\'i}ctor and Sommer, Lukas and Lom{\"u}ller, Victor and Narasimhan, Kumudha and Goli, Mehdi},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {21:1--21:25},
  issn = {1544-3566},
  doi = {10.1145/3571284},
  urldate = {2024-09-08},
  abstract = {Heterogeneous programming models are becoming increasingly popular to support the ever-evolving hardware architectures, especially for new and emerging specialized accelerators optimizing specific tasks. While such programs provide performance portability of the existing applications across various heterogeneous architectures to some extent, short-running device kernels can affect an application performance due to overheads of data transfer, synchronization, and kernel launch. While in applications with one or two short-running kernels the overhead can be negligible, it can be noticeable when these short-running kernels dominate the overall number of kernels in an application, as it is the case in graph-based neural network models, where there are several small memory-bound nodes alongside few large compute-bound nodes. To reduce the overhead, combining several kernels into a single, more optimized kernel is an active area of research. However, this task can be time-consuming and error-prone given the huge set of potential combinations. This can push programmers to seek a tradeoff between (a) task-specific kernels with low overhead but hard to maintain and (b) smaller modular kernels with higher overhead but easier to maintain. While there are DSL-based approaches, such as those provided for machine learning frameworks, which offer the possibility of such a fusion, they are limited to a particular domain and exploit specific knowledge of that domain and, as a consequence, are hard to port elsewhere. This study explores the feasibility of a user-driven kernel fusion through an extension to the SYCL API to address the automation of kernel fusion. The proposed solution requires programmers to define the subgraph regions that are potentially suitable for fusion without any modification to the kernel code or the function signature. We evaluate the performance benefit of our approach on common neural networks and study the performance improvement in detail.},
  note = {1:00, Trasahit, Talks about the parallel programming appeoch from kernel's perpective.},
  file = {/Users/ilu/Zotero/storage/3WBIF7IK/Prez et al. - 2023 - User-driven Online Kernel Fusion for SYCL.pdf}
}

@article{puthoorTurnbasedSpatiotemporalCoherence2023,
  title = {Turn-Based {{Spatiotemporal Coherence}} for {{GPUs}}},
  author = {Puthoor, Sooraj and Lipasti, Mikko H.},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {33:1--33:27},
  issn = {1544-3566},
  doi = {10.1145/3593054},
  urldate = {2024-09-08},
  abstract = {This article introduces turn-based spatiotemporal coherence. Spatiotemporal coherence is a novel coherence implementation that assigns write permission to epochs (or turns) as opposed to a processor core. This paradigm shift in the assignment of write permissions satisfies all conditions of a coherence protocol with virtually no coherence overhead. We discuss the implementation of this coherence mechanism on a baseline GPU. The evaluation shows that spatiotemporal coherence achieves a speedup of 7.13\% for workloads with read data reuse across kernels compared to the baseline software-managed GPU coherence implementation while also providing write atomicity and avoiding the need for software inserted acquire-release operations.1},
  note = {1:30, trashit, spatiotemporal coherence in caches, a very interesting read, you should try.},
  file = {/Users/ilu/Zotero/storage/TPIS6QNZ/Puthoor and Lipasti - 2023 - Turn-based Spatiotemporal Coherence for GPUs.pdf}
}

@article{raviTNTModularApproach2023,
  title = {{{TNT}}: {{A Modular Approach}} to {{Traversing Physically Heterogeneous NOCs}} at {{Bare-wire Latency}}},
  shorttitle = {{{TNT}}},
  author = {Ravi, Gokul Subramanian and Krishna, Tushar and Lipasti, Mikko},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {35:1--35:25},
  issn = {1544-3566},
  doi = {10.1145/3597611},
  urldate = {2024-09-08},
  abstract = {The ideal latency for on-chip network traversal would be the delay incurred from wire traversal alone. Unfortunately, in a realistic modular network, the latency for a packet to traverse the network is significantly higher than this wire delay. The main limiter to achieving lower latency is the modular quantization of network traversal into hops. Beyond this, the physical heterogeneity in real-world systems further complicate the ability to reach ideal wire-only delay.In this work, we propose TNT or Transparent Network Traversal. TNT targets ideal network latency by attempting source to destination network traversal as a single multi-cycle `long-hop', bypassing the quantization effects of intermediate routers via transparent data/information flow. TNT is built in a modular tile-scalable manner via a novel control path performing neighbor-to-neighbor interactions but enabling end-to-end transparent flit traversal. Further, TNT's fine grained on-the-fly delay tracking allows it to cope with physical NOC heterogeneity across the chip.Analysis on Ligra graph workloads shows that TNT can reduce NOC latency by as much as 43\% compared to the state of the art and allows efficiency gains up to 38\%. Further, it can achieve more than 3x the benefits of the best/closest alternative research proposal, SMART\&nbsp;[43].},
  note = {1:30, Not sure about this, talks about esthablishing a communication networks between the many networks of exa scale systems.},
  file = {/Users/ilu/Zotero/storage/3DMLY7BH/Ravi et al. - 2023 - TNT A Modular Approach to Traversing Physically Heterogeneous NOCs at Bare-wire Latency.pdf}
}

@article{reberCacheProgrammingScientific2023,
  title = {Cache {{Programming}} for {{Scientific Loops Using Leases}}},
  author = {Reber, Benjamin and Gould, Matthew and Kneipp, Alexander H. and Liu, Fangzhou and Prechtl, Ian and Ding, Chen and Chen, Linlin and Patru, Dorin},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {39:1--39:25},
  issn = {1544-3566},
  doi = {10.1145/3600090},
  urldate = {2024-09-08},
  abstract = {Cache management is important in exploiting locality and reducing data movement. This article studies a new type of programmable cache called the lease cache. By assigning leases, software exerts the primary control on when and how long data stays in the cache. Previous work has shown an optimal solution for an ideal lease cache.This article develops and evaluates a set of practical solutions for a physical lease cache emulated in FPGA with the full suite of PolyBench benchmarks. Compared to automatic caching, lease programming can further reduce data movement by 10\% to over 60\% when the data size is 16 times to 3,000 times the cache size, and the techniques in this article realize over 80\% of this potential. Moreover, lease programming can reduce data movement by another 0.8\% to 20\% after polyhedral locality optimization.},
  note = {1:30, treashit, talks about securing the cache in GPU and CPU using leasing approach. Very interesting.},
  file = {/Users/ilu/Zotero/storage/IF2SNSTR/Reber et al. - 2023 - Cache Programming for Scientific Loops Using Leases.pdf}
}

@article{sahniASMAdaptiveSecure2023,
  title = {{{ASM}}: {{An Adaptive Secure Multicore}} for {{Co-located Mutually Distrusting Processes}}},
  shorttitle = {{{ASM}}},
  author = {Sahni, Abdul Rasheed and Omar, Hamza and Ali, Usman and Khan, Omer},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {32:1--32:24},
  issn = {1544-3566},
  doi = {10.1145/3587480},
  urldate = {2024-09-08},
  abstract = {With the ever-increasing virtualization of software and hardware, the privacy of user-sensitive data is a fundamental concern in computation outsourcing. Secure processors enable a trusted execution environment to guarantee security properties based on the principles of isolation, sealing, and integrity. However, the shared hardware resources within the microarchitecture are increasingly being used by co-located adversarial software to create timing-based side-channel attacks. State-of-the-art secure processors implement the strong isolation primitive to enable non-interference for shared hardware but suffer from frequent state purging and resource utilization overheads, leading to degraded performance. This article proposes ASM, an adaptive secure multicore architecture that enables a reconfigurable, yet strongly isolated execution environment. For outsourced security-critical processes, the proposed security kernel and hardware extensions allow either a given process to execute using all available cores or co-execute multiple processes on strongly isolated clusters of cores. This spatio-temporal execution environment is configured based on resource demands of processes, such that the secure processor mitigates state purging overheads and maximizes hardware resource utilization.},
  note = {2:00, scanit, secure orocessor ddesign for evaluating the execution processes in distributed computingn paradigm and virtualizzation too.},
  file = {/Users/ilu/Zotero/storage/RXRV5F7I/Sahni et al. - 2023 - ASM An Adaptive Secure Multicore for Co-located Mutually Distrusting Processes.pdf}
}

@article{sakalisDelaySquashStoppingMicroarchitectural2022,
  title = {Delay-on-{{Squash}}: {{Stopping Microarchitectural Replay Attacks}} in {{Their Tracks}}},
  shorttitle = {Delay-on-{{Squash}}},
  author = {Sakalis, Christos and Kaxiras, Stefanos and Sj{\"a}lander, Magnus},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {9:1--9:24},
  issn = {1544-3566},
  doi = {10.1145/3563695},
  urldate = {2024-09-08},
  abstract = {MicroScope and other similar microarchitectural replay attacks take advantage of the characteristics of speculative execution to trap the execution of the victim application in a loop, enabling the attacker to amplify a side-channel attack by executing it indefinitely. Due to the nature of the replay, it can be used to effectively attack software that are shielded against replay, even under conditions where a side-channel attack would not be possible (e.g., in secure enclaves). At the same time, unlike speculative side-channel attacks, microarchitectural replay attacks can be used to amplify the correct path of execution, rendering many existing speculative side-channel defenses ineffective. In this work, we generalize microarchitectural replay attacks beyond MicroScope and present an efficient defense against them. We make the observation that such attacks rely on repeated squashes of so-called ``replay handles'' and that the instructions causing the side-channel must reside in the same reorder buffer window as the handles. We propose Delay-on-Squash, a hardware-only technique for tracking squashed instructions and preventing them from being replayed by speculative replay handles. Our evaluation shows that it is possible to achieve full security against microarchitectural replay attacks with very modest hardware requirements while still maintaining 97\% of the insecure baseline performance.},
  note = {Critical REad, 45 sec, Scan it, Talks about halting/disallowing speculative execution after page miss, very closely related to your work.The poinnt is not to significantly impact the performance but to stop replay attacks by tracking the squashes. Bloom filters are suedd to deteect if a PC is ina set after hashing it if yes, then a bulk reset is the only way to reset the filter which will change the pc. approach is to ise two bloom fileters and switch between them. at ny point in time only fileter is tagged as active. the other one will wait to be cleard. after the the active filter is cleared the status is altered on both filters. take figure 4 for IPC metric.},
  file = {/Users/ilu/Zotero/storage/7U7486BW/Sakalis et al. - 2022 - Delay-on-Squash Stopping Microarchitectural Replay Attacks in Their Tracks.pdf}
}

@article{schulerXEngineOptimalTensor2022,
  title = {{{XEngine}}: {{Optimal Tensor Rematerialization}} for {{Neural Networks}} in {{Heterogeneous Environments}}},
  shorttitle = {{{XEngine}}},
  author = {Schuler, Manuela and Membarth, Richard and Slusallek, Philipp},
  year = {2022},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {17:1--17:25},
  issn = {1544-3566},
  doi = {10.1145/3568956},
  urldate = {2024-09-08},
  abstract = {Memory efficiency is crucial in training deep learning networks on resource-restricted devices. During backpropagation, forward tensors are used to calculate gradients. Despite the option of keeping those dependencies in memory until they are reused in backpropagation, some forward tensors can be discarded and recomputed later from saved tensors, so-called checkpoints. This allows, in particular, for resource-constrained heterogeneous environments to make use of all available compute devices. Unfortunately, the definition of these checkpoints is a non-trivial problem and poses a challenge to the programmer---improper or excessive recomputations negate the benefit of checkpointing. \&nbsp;\&nbsp; In this article, we present XEngine, an approach that schedules network operators to heterogeneous devices in low memory environments by determining checkpoints and recomputations of tensors. Our approach selects suitable resources per timestep and operator and optimizes the end-to-end time for neural networks taking the memory limitation of each device into account. For this, we formulate a mixed-integer quadratic program (MIQP) to schedule operators of deep learning networks on heterogeneous systems. We compare our MIQP solver XEngine against Checkmate\&nbsp;[12], a mixed-integer linear programming (MILP) approach that solves recomputation on a single device. Our solver finds solutions that are up to 22.5\% faster than the fastest Checkmate schedule in which the network is computed exclusively on a single device. We also find valid schedules for networks making use of both central processing units and graphics processing units if memory limitations do not allow scheduling exclusively to the graphics processing unit.},
  note = {1:12, Trash it, talks about memory effeciency in tensors.},
  file = {/Users/ilu/Zotero/storage/QV6KY2IJ/Schuler et al. - 2022 - XEngine Optimal Tensor Rematerialization for Neural Networks in Heterogeneous Environments.pdf}
}

@article{shahBullsEyeScalableAccurate2022,
  title = {{{BullsEye}} : {{Scalable}} and {{Accurate Approximation Framework}} for {{Cache Miss Calculation}}},
  shorttitle = {{{BullsEye}}},
  author = {Shah, Nilesh Rajendra and Misra, Ashitabh and Min{\'e}, Antoine and Venkat, Rakesh and Upadrasta, Ramakrishna},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {2:1--2:28},
  issn = {1544-3566},
  doi = {10.1145/3558003},
  urldate = {2024-09-08},
  abstract = {For Affine Control Programs or Static Control Programs (SCoP), symbolic counting of reuse distances could induce polynomials for each reuse pair. These polynomials along with cache capacity constraints lead to non-affine (semi-algebraic) sets; and counting these sets is considered to be a hard problem. The state-of-the-art methods use various exact enumeration techniques relying on existing cardinality algorithms that can efficiently count affine sets.We propose BullsEye , a novel, scalable, accurate, and problem-size independent approximation framework. It is an analytical cache model for fully associative caches with LRU replacement policy focusing on sampling and linearization of non-affine stack distance polynomials. First, we propose a simple domain sampling method that can improve the scalability of exact enumeration. Second, we propose linearization techniques relying on Handelman's theorem and Bernstein's representation. To improve the scalability of the Handelman's theorem linearization technique, we propose template (Interval or Octagon) sub-polyhedral approximations.Our methods obtain significant compile-time improvements with high-accuracy when compared to HayStack on important polyhedral compilation kernels such as nussinov, cholesky, and adi from PolyBench, and harris, gaussianblur from LLVM-TestSuite. Overall, on PolyBench kernels, our methods show up to 3.31{\texttimes} (geomean) speedup with errors below {$\approx$} 0.08\% (geomean) for the octagon sub-polyhedral approximation.},
  note = {1:47, Scan it, Talks about cache miss calculations in the perspective of math resulting in an approach called BULLSEYE, could be useful to evaluate BTB for virtual addresses.},
  file = {/Users/ilu/Zotero/storage/62N9BVN7/Shah et al. - 2022 - BullsEye  Scalable and Accurate Approximation Framework for Cache Miss Calculation.pdf}
}

@article{shahTokenSmartDistributedScalable2022,
  title = {{{TokenSmart}}: {{Distributed}}, {{Scalable Power Management}} in the {{Many-core Era}}},
  shorttitle = {{{TokenSmart}}},
  author = {Shah, Parth and Shenoy, Ranjal Gautham and Srinivasan, Vaidyanathan and Bose, Pradip and Buyuktosunoglu, Alper},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {4:1--4:26},
  issn = {1544-3566},
  doi = {10.1145/3559762},
  urldate = {2024-09-08},
  abstract = {Centralized power management control systems are hitting a scalability limit. In particular, enforcing a power cap in a many-core system in a performance-friendly manner is quite challenging. Today's on-chip controller reduces the clock speed of compute domains in response to local or global power limit alerts. However, this is opaque to the operating system (OS), which continues to request higher clock frequency based on the workload characteristics acting against the centralized on-chip controller. To address these issues, we introduce TokenSmart, which implements a set of scalable distributed frequency control heuristics within the OS, using a novel token-based mechanism. The number of system-allocated power tokens represents the maximum allowable power consumption; and the OS governor orchestrates a token-passing (or sharing) algorithm between the compute engines. Token allocation count increase (decrease) corresponds to a increase (decrease) of clock frequency. The compute units are connected in a ring-topology allowing minimal meta-data to be passed along with the token value for regulating power budget. We explore different heuristics to assign tokens smartly across the units. This results in efficient power regulation and sustenance of turbo frequencies over a longer duration. Our proposed methodology can be implemented in hardware with multiple on-chip controllers, or in software where each set of cores acts as a compute unit. The methodology is currently implemented within the Linux kernel of a real IBM POWER9 many-core system and experimentally verified on different real world workloads such as Redis, Cassandra, PostgreSQL along with a micro-benchmark such as rt-app. Our experiments indicate the increase in throughput for all the workloads along with the benefit of power savings. For instance, results show a considerable boost of about 4\% in throughput of both the PostgreSQL and Redis benchmark with a substantial savings in power consumption (18\% and 37\%, respectively). If the approach is implemented in hardware, then our experimental analysis speculates the throughput to increase up to 14\% in PostgreSQL benchmark.},
  note = {1:12, Trash it, Talks about power management in multi-core systems.},
  file = {/Users/ilu/Zotero/storage/WP93CKTI/Shah et al. - 2022 - TokenSmart Distributed, Scalable Power Management in the Many-core Era.pdf}
}

@article{singhHyGainHighperformanceEnergyefficient2023,
  title = {{{HyGain}}: {{High-performance}}, {{Energy-efficient Hybrid Gain Cell-based Cache Hierarchy}}},
  shorttitle = {{{HyGain}}},
  author = {Singh, Sarabjeet and Surana, Neelam and Prasad, Kailash and Jain, Pranjali and Mekie, Joycee and Awasthi, Manu},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {24:1--24:20},
  issn = {1544-3566},
  doi = {10.1145/3572839},
  urldate = {2024-09-08},
  abstract = {In this article, we propose a ``full-stack'' solution to designing high-apacity and low-latency on-chip cache hierarchies by starting at the circuit level of the hardware design stack. We propose a novel half VDD precharge 2T Gain Cell (GC) design for the cache hierarchy. The GC has several desirable characteristics, including {\textasciitilde}50\% higher storage density and {\textasciitilde}50\% lower dynamic energy as compared to the traditional 6T SRAM, even after accounting for peripheral circuit overheads. We also demonstrate data retention time of 350 us ({\textasciitilde}17.5{\texttimes} of eDRAM) at 28 nm technology with VDD = 0.9V and temperature = 27{$^\circ$}C that, combined with optimizations like staggered refresh, makes it an ideal candidate to architect all levels of on-chip caches. We show that compared to 6T SRAM, for a given area budget, GC-based caches, on average, provide 30\% and 36\% increase in IPC for single- and multi-programmed workloads, respectively, on contemporary workloads, including SPEC CPU 2017. We also observe dynamic energy savings of 42\% and 34\% for single- and multi-programmed workloads, respectively. Finally, in a quest to utilize the best of all worlds, we combine GC with STT-RAM to create hybrid hierarchies. We show that a hybrid hierarchy with GC caches at L1 and L2 and an LLC split between GC and STT-RAM is able to provide a 46\% benefit in energy-delay product (EDP) as compared to an all-SRAM design, and 13\% as compared to an all-GC cache hierarchy, averaged across multi-programmed workloads.},
  note = {1:00, Traash it, NOt relatd.},
  file = {/Users/ilu/Zotero/storage/QUUMMW2P/Singh et al. - 2023 - HyGain High-performance, Energy-efficient Hybrid Gain Cell-based Cache Hierarchy.pdf}
}

@article{soniApproximateComputing2022,
  title = {As-{{Is Approximate Computing}}},
  author = {Soni, Mitali and Pal, Asmita and Miguel, Joshua San},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {3:1--3:26},
  issn = {1544-3566},
  doi = {10.1145/3559761},
  urldate = {2024-09-08},
  abstract = {Although approximate computing promises better performance for applications allowing marginal errors, dearth of hardware support and lack of run-time accuracy guarantees makes it difficult to adopt. We present As-Is, an Anytime Speculative Interruptible System that takes an approximate program and executes it with time-proportional approximations. That is, an approximate version of the program output is generated early and is gradually refined over time, thus providing the run-time guarantee of eventually reaching 100\% accuracy. The novelty of our As-Is architecture is in its ability to conceptually marry approximate computing and speculative computing. We show how existing innovations in speculative architectures can be repurposed for anytime, best-effort approximation, facilitating the design efforts and overheads needed for approximate hardware support. As-Is provides a platform for real-time constraints and interactive users to interrupt programs early and accept their current approximate results as is. 100\% accuracy is always guaranteed if more time can be spared. Our evaluations demonstrate favorable performance-accuracy tradeoffs for a range of approximate applications.},
  note = {2:00, Scan it, Approximation of execution output and talks about architecture, potential interesting read.},
  file = {/Users/ilu/Zotero/storage/H3KBPYMD/Soni et al. - 2022 - As-Is Approximate Computing.pdf}
}

@article{tollenaereAutotuningConvolutionsEasier2023,
  title = {Autotuning {{Convolutions Is Easier Than You Think}}},
  author = {Tollenaere, Nicolas and Iooss, Guillaume and Pouget, St{\'e}phane and Brunie, Hugo and Guillon, Christophe and Cohen, Albert and Sadayappan, P. and Rastello, Fabrice},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {20:1--20:24},
  issn = {1544-3566},
  doi = {10.1145/3570641},
  urldate = {2024-09-08},
  abstract = {A wide range of scientific and machine learning applications depend on highly optimized implementations of tensor computations. Exploiting the full capacity of a given processor architecture remains a challenging task, due to the complexity of the microarchitectural features that come into play when seeking near-peak performance. Among the state-of-the-art techniques for loop transformations for performance optimization, AutoScheduler\&nbsp;[Zheng et\&nbsp;al. 2020a] tends to outperform other systems. It often yields higher performance as compared to vendor libraries, but takes a large number of runs to converge, while also involving a complex training environment. In this article, we define a structured configuration space that enables much faster convergence to high-performance code versions, using only random sampling of candidates. We focus on two-dimensional convolutions on CPUs. Compared to state-of-the-art libraries, our structured search space enables higher performance for typical tensor shapes encountered in convolution stages in deep learning pipelines. Compared to auto-tuning code generators like AutoScheduler, it prunes the search space while increasing the density of efficient implementations. We analyze the impact on convergence speed and performance distribution, on two Intel x86 processors and one ARM AArch64 processor. We match or outperform the performance of the state-of-the-art oneDNN library and TVM's AutoScheduler, while reducing the autotuning effort by at least an order of magnitude.},
  note = {30 secs, trash it, Not related.},
  file = {/Users/ilu/Zotero/storage/72VXWLCW/Tollenaere et al. - 2023 - Autotuning Convolutions Is Easier Than You Think.pdf}
}

@article{xieMPUMemorycentricSIMT2023,
  title = {{{MPU}}: {{Memory-centric SIMT Processor}} via {{In-DRAM Near-bank Computing}}},
  shorttitle = {{{MPU}}},
  author = {Xie, Xinfeng and Gu, Peng and Ding, Yufei and Niu, Dimin and Zheng, Hongzhong and Xie, Yuan},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {40:1--40:26},
  issn = {1544-3566},
  doi = {10.1145/3603113},
  urldate = {2024-09-08},
  abstract = {With the growing number of data-intensive workloads, GPU, which is the state-of-the-art single-instruction-multiple-thread (SIMT) processor, is hindered by the memory bandwidth wall. To alleviate this bottleneck, previously proposed 3D-stacking near-bank computing accelerators benefit from abundant bank-internal bandwidth by bringing computations closer to the DRAM banks. However, these accelerators are specialized for certain application domains with simple architecture data paths and customized software mapping schemes. For general-purpose scenarios, lightweight hardware designs for diverse data paths, architectural supports for the SIMT programming model, and end-to-end software optimizations remain challenging.To address these issues, we propose Memory-centric Processing Unit (MPU), the first SIMT processor based on 3D-stacking near-bank computing architecture. First, to realize diverse data paths with small overheads, MPU adopts a hybrid pipeline with the capability of offloading instructions to near-bank compute-logic. Second, we explore two architectural supports for the SIMT programming model, including a near-bank shared memory design and a multiple activated row-buffers enhancement. Third, we present an end-to-end compilation flow for MPU to support CUDA programs. To fully utilize MPU's hybrid pipeline, we develop a backend optimization for the instruction offloading decision. The evaluation results of MPU demonstrate 3.46{\texttimes} speedup and 2.57{\texttimes} energy reduction compared with an NVIDIA Tesla V100 GPU on a set of representative data-intensive workloads.},
  note = {2:00, Scan it, Relatedd to the other paper in issue 2 with near memory computoing.},
  file = {/Users/ilu/Zotero/storage/D45K5TZ5/Xie et al. - 2023 - MPU Memory-centric SIMT Processor via In-DRAM Near-bank Computing.pdf}
}

@article{xuAcceleratingConvolutionalNeural2023,
  title = {Accelerating {{Convolutional Neural Network}} by {{Exploiting Sparsity}} on {{GPUs}}},
  author = {Xu, Weizhi and Sun, Yintai and Fan, Shengyu and Yu, Hui and Fu, Xin},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {36:1--36:26},
  issn = {1544-3566},
  doi = {10.1145/3600092},
  urldate = {2024-09-08},
  abstract = {The convolutional neural network (CNN) is an important deep learning method, which is widely used in many fields. However, it is very time consuming to implement the CNN where convolution usually takes most of the time. There are many zero values in feature maps and filters, which leads to redundant calculations and memory accesses if dense methods are used to compute convolution. Many works recently have made use of sparsity to skip the calculations for zero values to reduce the inference time of the CNN. On the graphics processing unit platform, current works cannot fully exploit the sparsity of the feature map and achieve satisfactory performance. Therefore, we design a new parallel strategy to transform the feature map into a new storage format to avoid the redundant computation of zero values on graphics processing units. Also considering the sparsity in the feature map, we propose a fused storage format to combine the convolution operation with the following pooling operation, to further improve the performance. We carry out experiments with mainstream CNN models and achieve better performance compared with cuDNN and cuSPARSE. For VGG-19, ResNet-50, DenseNet-121, and RegNetX-16GF, 1.97{\texttimes}, 2.23{\texttimes}, 2.74{\texttimes}, and 1.58{\texttimes} speedups respectively are obtained over cuDNN. The speedups over cuSPARSE respectively are 2.10{\texttimes}, 1.83{\texttimes}, 2.35{\texttimes}, and 1.35{\texttimes} when only using the first method.},
  note = {30 secs, Trash it, Not related.},
  file = {/Users/ilu/Zotero/storage/56QGQNLC/Xu et al. - 2023 - Accelerating Convolutional Neural Network by Exploiting Sparsity on GPUs.pdf}
}

@article{xuSSDSGDCommunicationSparsification2022,
  title = {{{SSD-SGD}}: {{Communication Sparsification}} for {{Distributed Deep Learning Training}}},
  shorttitle = {{{SSD-SGD}}},
  author = {Xu, Yemao and Dong, Dezun and Wang, Dongsheng and Xu, Shi and Yu, Enda and Xu, Weixia and Liao, Xiangke},
  year = {2022},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {7:1--7:25},
  issn = {1544-3566},
  doi = {10.1145/3563038},
  urldate = {2024-09-08},
  abstract = {Intensive communication and synchronization cost for gradients and parameters is the well-known bottleneck of distributed deep learning training. Based on the observations that Synchronous SGD (SSGD) obtains good convergence accuracy while asynchronous SGD (ASGD) delivers a faster raw training speed, we propose Several Steps Delay SGD (SSD-SGD) to combine their merits, aiming at tackling the communication bottleneck via communication sparsification. SSD-SGD explores both global synchronous updates in the parameter servers and asynchronous local updates in the workers in each periodic iteration. The periodic and flexible synchronization makes SSD-SGD achieve good convergence accuracy and fast training speed. To the best of our knowledge, we strike the new balance between synchronization quality and communication sparsification, and improve the tradeoff between accuracy and training speed. Specifically, the core components of SSD-SGD include proper warm-up stage, steps delay stage, and the novel algorithm of global gradient for local update (GLU). GLU is critical for local update operations by using global gradient information to effectively compensate for the delayed local weights. Furthermore, we implement SSD-SGD on MXNet framework and comprehensively evaluate its performance with CIFAR-10 and ImageNet datasets. Experimental results show that SSD-SGD can accelerate distributed training speed under different experimental configurations, by up to 110\% (or 2.1{\texttimes} of the original speed), while achieving good convergence accuracy.},
  note = {2:00, scan, Contains a theory about Data Parallelism might be useful.},
  file = {/Users/ilu/Zotero/storage/J5TTKW9F/Xu et al. - 2022 - SSD-SGD Communication Sparsification for Distributed Deep Learning Training.pdf}
}

@article{yuzugulerScaleoutSystolicArrays2023,
  title = {Scale-out {{Systolic Arrays}}},
  author = {Y{\"u}z{\"u}g{\"u}ler, Ahmet Caner and S{\"o}nmez, Canberk and Drumond, Mario and Oh, Yunho and Falsafi, Babak and Frossard, Pascal},
  year = {2023},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {2},
  pages = {27:1--27:25},
  issn = {1544-3566},
  doi = {10.1145/3572917},
  urldate = {2024-09-08},
  abstract = {Multi-pod systolic arrays are emerging as the architecture of choice in DNN inference accelerators. Despite their potential, designing multi-pod systolic arrays to maximize effective throughput/Watt---i.e., throughput/Watt adjusted when accounting for array utilization---poses a unique set of challenges. In this work, we study three key pillars in multi-pod systolic array designs, namely array granularity, interconnect, and tiling. We identify optimal array granularity across workloads and show that state-of-the-art commercial accelerators use suboptimal array sizes for single-tenancy workloads. We, then evaluate the bandwidth/latency trade-offs in interconnects and show that Butterfly networks offer a scalable topology for accelerators with a large number of pods. Finally, we introduce a novel data tiling scheme with custom partition size to maximize utilization in optimally sized pods. We propose Scale-out Systolic Arrays, a multi-pod inference accelerator for both single- and multi-tenancy based on these three pillars. We show that SOSA exhibits scaling of up to 600\&nbsp;TeraOps/s in effective throughput for state-of-the-art DNN inference workloads, and outperforms state-of-the-art multi-pod accelerators by a factor of 1.5 {\texttimes}.1},
  note = {25, Trashit, Not related.},
  file = {/Users/ilu/Zotero/storage/LNIA849X/Yzgler et al. - 2023 - Scale-out Systolic Arrays.pdf}
}

@article{zhangRegCPythonRegisterbasedPython2022,
  title = {{{RegCPython}}: {{A Register-based Python Interpreter}} for {{Better Performance}}},
  shorttitle = {{{RegCPython}}},
  author = {Zhang, Qiang and Xu, Lei and Xu, Baowen},
  year = {2022},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {14:1--14:25},
  issn = {1544-3566},
  doi = {10.1145/3568973},
  urldate = {2024-09-08},
  abstract = {Interpreters are widely used in the implementation of many programming languages, such as Python, Perl, and Java. Even though various JIT compilers emerge in an endless stream, interpretation efficiency still plays a critical role in program performance. Does a stack-based interpreter or a register-based interpreter perform better? The pros and cons of the pair of architectures have long been discussed. The stack architecture is attractive for its concise model and compact bytecode, but our study finds that the register-based interpreter can also be implemented easily and that its bytecode size only grows by a small margin. Moreover, the latter turns out to be appreciably faster. Specifically, we implemented an open source Python interpreter named RegCPython based on CPython v3.10.1. The former is register based, while the latter is stack based. Without changes in syntax, Application Programming Interface, and Application Binary Interface, RegCPython is excellently compatible with CPython, as it does not break existing syntax or interfaces. It achieves a speedup of 1.287 on the most favorable benchmark and 0.977 even on the most unfavorable benchmark. For all Python-intensive benchmarks, the average speedup reaches 1.120 on x86 and 1.130 on ARM. Our evaluation work, which also serves as an empirical study, provides a detailed performance survey of both interpreters on modern hardware. It points out that the register-based interpreters are more efficient mainly due to the elimination of machine instructions needed, while changes in branch mispredictions and cache misses have a limited impact on performance. Additionally, it confirms that the register-based implementation is also satisfactory in terms of memory footprint, compilation cost, and implementation complexity.},
  note = {1:10, Trash it, register heavy Byte-code generation for python.},
  file = {/Users/ilu/Zotero/storage/5AERFETI/Zhang et al. - 2022 - RegCPython A Register-based Python Interpreter for Better Performance.pdf}
}

@article{zhaoGraphTuneEfficientDependencyAware2023,
  title = {{{GraphTune}}: {{An Efficient Dependency-Aware Substrate}} to {{Alleviate Irregularity}} in {{Concurrent Graph Processing}}},
  shorttitle = {{{GraphTune}}},
  author = {Zhao, Jin and Zhang, Yu and He, Ligang and Li, Qikun and Zhang, Xiang and Jiang, Xinyu and Yu, Hui and Liao, Xiaofei and Jin, Hai and Gu, Lin and Liu, Haikun and He, Bingsheng and Zhang, Ji and Song, Xianzheng and Wang, Lin and Zhou, Jun},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {37:1--37:24},
  issn = {1544-3566},
  doi = {10.1145/3600091},
  urldate = {2024-09-08},
  abstract = {With the increasing need for graph analysis, massive Concurrent iterative Graph Processing (CGP) jobs are usually performed on the common large-scale real-world graph. Although several solutions have been proposed, these CGP jobs are not coordinated with the consideration of the inherent dependencies in graph data driven by graph topology. As a result, they suffer from redundant and fragmented accesses of the same underlying graph dispersed over distributed platform, because the same graph is typically irregularly traversed by these jobs along different paths at the same time.In this work, we develop GraphTune, which can be integrated into existing distributed graph processing systems, such as D-Galois, Gemini, PowerGraph, and Chaos, to efficiently perform CGP jobs and enhance system throughput. The key component of GraphTune is a dependency-aware synchronous execution engine in conjunction with several optimization strategies based on the constructed cross-iteration dependency graph of chunks. Specifically, GraphTune transparently regularizes the processing behavior of the CGP jobs in a novel synchronous way and assigns the chunks of graph data to be handled by them based on the topological order of the dependency graph so as to maximize the performance. In this way, it can transform the irregular accesses of the chunks into more regular ones so that as many CGP jobs as possible can fully share the data accesses to the common graph. Meanwhile, it also efficiently synchronizes the communications launched by different CGP jobs based on the dependency graph to minimize the communication cost. We integrate it into four cutting-edge distributed graph processing systems and a popular out-of-core graph processing system to demonstrate the efficiency of GraphTune. Experimental results show that GraphTune improves the throughput of CGP jobs by 3.1{$\sim$}6.2, 3.8{$\sim$}8.5, 3.5{$\sim$}10.8, 4.3{$\sim$}12.4, and 3.8{$\sim$}6.9 times over D-Galois, Gemini, PowerGraph, Chaos, and GraphChi, respectively.},
  note = {30 secs, Trash it, Not related.},
  file = {/Users/ilu/Zotero/storage/JEEBRKFF/Zhao et al. - 2023 - GraphTune An Efficient Dependency-Aware Substrate to Alleviate Irregularity in Concurrent Graph Pro.pdf}
}

@article{zhaoMFFTGPUAccelerated2023,
  title = {{{MFFT}}: {{A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT Framework}}},
  shorttitle = {{{MFFT}}},
  author = {Zhao, Yuwen and Liu, Fangfang and Ma, Wenjing and Li, Huiyuan and Peng, Yuanchi and Wang, Cui},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {43:1--43:23},
  issn = {1544-3566},
  doi = {10.1145/3605148},
  urldate = {2024-09-08},
  abstract = {Fast Fourier transform (FFT) is widely used in computing applications in large-scale parallel programs, and data communication is the main performance bottleneck of FFT and seriously affects its parallel efficiency. To tackle this problem, we propose a new large-scale FFT framework, MFFT, which optimizes parallel FFT with a new mixed-precision optimization technique, adopting the ``high precision computation, low precision communication'' strategy. To enable ``low precision communication'', we propose a shared-exponent floating-point number compression technique, which reduces the volume of data communication, while maintaining higher accuracy. In addition, we apply a two-phase normalization technique to further reduce the round-off error. Based on the mixed-precision MFFT framework, we apply several optimization techniques to improve the performance, such as streaming of GPU kernels, MPI message combination, kernel optimization, and memory optimization. We evaluate MFFT on a system with 4,096 GPUs. The results show that shared-exponent MFFT is 1.23 {\texttimes} faster than that of double-precision MFFT on average, and double-precision MFFT achieves performance 3.53{\texttimes} and 9.48{\texttimes} on average higher than open source library 2Decomp\&amp;FFT (CPU-based version) and heFFTe (AMD GPU-based version), respectively. The parallel efficiency of double-precision MFFT increased from 53.2\% to 78.1\% compared with 2Decomp\&amp;FFT, and shared-exponent MFFT further increases the parallel efficiency to 83.8\%.},
  note = {55secs, Trashit, Noe related.},
  file = {/Users/ilu/Zotero/storage/V57B7QFH/Zhao et al. - 2023 - MFFT A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT Framework.pdf}
}

@article{zhaoPolyhedralSpecificationCode2022,
  title = {Polyhedral {{Specification}} and {{Code Generation}} of {{Sparse Tensor Contraction}} with {{Co-iteration}}},
  author = {Zhao, Tuowen and Popoola, Tobi and Hall, Mary and Olschanowsky, Catherine and Strout, Michelle},
  year = {2022},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {1},
  pages = {16:1--16:26},
  issn = {1544-3566},
  doi = {10.1145/3566054},
  urldate = {2024-09-08},
  abstract = {This article presents a code generator for sparse tensor contraction computations. It leverages a mathematical representation of loop nest computations in the sparse polyhedral framework (SPF), which extends the polyhedral model to support non-affine computations, such as those that arise in sparse tensors. SPF is extended to perform layout specification, optimization, and code generation of sparse tensor code: (1) We develop a polyhedral layout specification that decouples iteration spaces for layout and computation; and (2) we develop efficient co-iteration of sparse tensors by combining polyhedra scanning over the layout of one sparse tensor with the synthesis of code to find corresponding elements in other tensors through an SMT solver.We compare the generated code with that produced by a state-of-the-art tensor compiler, TACO. We achieve on average 1.63{\texttimes} faster parallel performance than TACO on sparse-sparse co-iteration and describe how to improve that to 2.72{\texttimes} average speedup by switching the find algorithms. We also demonstrate that decoupling iteration spaces of layout and computation enables additional layout and computation combinations to be supported.},
  note = {1:30, trash it, Related to a different compilation technique.},
  file = {/Users/ilu/Zotero/storage/WRVJY3KQ/Zhao et al. - 2022 - Polyhedral Specification and Code Generation of Sparse Tensor Contraction with Co-iteration.pdf}
}

@article{zhouImpactPageSize2023,
  title = {The {{Impact}} of {{Page Size}} and {{Microarchitecture}} on {{Instruction Address Translation Overhead}}},
  author = {Zhou, Yufeng and Cox, Alan L. and Dwarkadas, Sandhya and Dong, Xiaowan},
  year = {2023},
  month = jul,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {20},
  number = {3},
  pages = {38:1--38:25},
  issn = {1544-3566},
  doi = {10.1145/3600089},
  urldate = {2024-09-08},
  abstract = {As the volume of data processed by applications has increased, considerable attention has been paid to data address translation overheads, leading to the widespread use of larger page sizes (``superpages'') and multi-level translation lookaside buffers (TLBs). However, far less attention has been paid to instruction address translation and its relation to TLB and pipeline structure. In prior work, we quantified the impact of using code superpages on a variety of widely used applications, ranging from compilers to web user-interface frameworks, and the impact of sharing page table pages for executables and shared libraries. Within this article, we augment those results by first uncovering the effects that microarchitectural differences between Intel Skylake and AMD Zen+, particularly their different TLB organizations, have on instruction address translation overhead. This analysis provides some key insights into the microarchitectural design decisions that impact the cost of instruction address translation. First, a lower-level (level 2) TLB that has both instruction and data mappings competing for space within the same structure allows better overall performance and utilization when using code superpages. Code superpages not only reduce instruction address translation overhead but also indirectly reduce data address translation overhead. In fact, for a few applications, the use of just a few code superpages has a larger impact on overall performance than the use of a much larger number of data superpages. Second, a level 1 (L1) TLB with separate structures for different page sizes may require careful tuning of the superpage promotion policy for code, and a correspondingly suboptimal utilization of the level 2 TLB. In particular, increasing the number of superpages when the size of the L1 superpage structure is small may result in more L1 TLB misses for some applications. Moreover, on some microarchitectures, the cost of these misses can be highly variable, because replacement is delayed until all of the in-flight instructions mapped by the victim entry are retired. Hence, more superpage promotions can result in a performance regression. Finally, our findings also make a case for first-class OS support for superpages on ordinary files containing executables and shared libraries, as well as a more aggressive superpage policy for code.},
  note = {Critical rEad, 2:00, Scan it, Talks about how page size and pipeline can impact hardware overhead of address translation. Quatification of different TLB organization based on overhead. TLB overhead is about 13.44\% of execution cycles. taken experimental setups XEon and Ryzen Free BSD OS. table 1 for TLB structures. Fig 1 for comparisions with differnt workloads.},
  file = {/Users/ilu/Zotero/storage/W2QLC9DI/Zhou et al. - 2023 - The Impact of Page Size and Microarchitecture on Instruction Address Translation Overhead.pdf}
}
